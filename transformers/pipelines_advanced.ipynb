{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3297bcf-728c-4910-9f94-6f9ecb1f9155",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines 进阶\n",
    "\n",
    "我们已经学习了 Pipeline API 针对各类任务的基本使用。\n",
    "\n",
    "实际上，在 Transformers 库内部实现中，Pipeline 作为管理：`原始文本-输入Token IDs-模型推理-输出概率-生成结果` 的流水线抽象，背后串联了 Transformers 库的核心模块 `Tokenizer`和 `Models`。\n",
    "\n",
    "![](docs/images/pipeline_advanced.png)\n",
    "\n",
    "下面我们开始结合大语言模型（在 Transformers 中也是一种特定任务）学习：\n",
    "\n",
    "- 使用 Pipeline 如何与现代的大语言模型结合，以完成各类下游任务\n",
    "- 使用 Tokenizer 编解码文本\n",
    "- 使用 Models 加载和保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e24b6-1c28-4729-8b7d-30f374c2b43d",
   "metadata": {},
   "source": [
    "## 使用 Pipeline 调用大语言模型\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "语言建模是一项预测文本序列中的单词的任务。它已经成为非常流行的自然语言处理任务，因为预训练的语言模型可以用于许多其他下游任务的微调。最近，对大型语言模型（LLMs）产生了很大兴趣，这些模型展示了零或少量样本学习能力。这意味着该模型可以解决其未经明确训练过的任务！虽然语言模型可用于生成流畅且令人信服的文本，但需要小心使用，因为文本可能并不总是准确无误。\n",
    "\n",
    "通过理论篇学习，我们了解到有两种典型的语言模型：\n",
    "\n",
    "- 自回归：模型目标是预测序列中的下一个 Token（文本），训练时对下文进行了掩码。如：GPT-3。\n",
    "- 自编码：模型目标是理解上下文后，补全句子中丢失/掩码的 Token（文本）。如：BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c2701-f414-48e4-8277-0c5483ba9717",
   "metadata": {},
   "source": [
    "### 使用 GPT-2 实现文本生成\n",
    "\n",
    "![](docs/images/gpt2.png)\n",
    "\n",
    "模型主页：https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c8fbd0-4301-4409-a0c3-f64ef92f08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba566e4-b8c1-44c2-a329-f6a3961a6b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4e4754eb70458ea1c818786d1bb2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60332\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\60332\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbceccf89534fa1952c7b07f5761531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21c7c27fe044aefbd5c783b597f726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0387aa7f58414f3885e5be150fda941e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f556b6b6ccbd45e3a3e07409e78c889c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hugging Face is a community-based open-source platform for machine learning. The developer behind the platform, Zellwelder, believes that his community (including employees) are most comfortable and productive when working with machines with high levels of abstraction,'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n",
    "\n",
    "generator = pipeline(task='text-generation', model='gpt2')\n",
    "\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005e433-2dfa-460c-89ea-9421a71c1b01",
   "metadata": {},
   "source": [
    "#### 设置文本生成返回条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab36bcd2-4ddf-4774-9ddb-70a6c6a334d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'You are very smart'\n",
    "generator = pipeline(task='text-generation', model='gpt2', num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2874d4-ffbc-4234-81d5-1a5bc9e96b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"You are very smart. You will always know your enemy. You may also be a genius. But you are also a great fool.\\n\\nThat's how I understood it—that the greatest person I ever known was a cunning fool.\\n\\n\"},\n",
       " {'generated_text': \"You are very smart, so we would appreciate your help with my research! You might be asking yourself: Is this right? Well, I think now, you probably think you're really smart, and your job is to help make it happen (it\"},\n",
       " {'generated_text': 'You are very smart and smart and I will get it.\"\\n\\nYou might have thought that you\\'d have some sort of connection with the other man in the photo, but that\\'s not quite the case.\\n\\nYou might have heard that his'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d94020b-6b01-4970-8a2f-14fc155f7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart and you can tell the difference between a person who goes blind and a person that makes history by looking at his or her face and saying, \"Thank you, my good son.\" I remember very well being surrounded by people who were'},\n",
       " {'generated_text': 'You are very smart! You have the information. Your browser is not secure.\"\\n\\nShe replied, \"No matter how many times you have answered that question, you will be unable to answer without the information. This is a simple case. I'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b4b1a-461f-4469-a01e-9c235caf9fa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 设置文本生成最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb44ccfb-3229-4d23-92b0-1e95b7f86b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart,\" she said.\\n\\nThe students also have a long'},\n",
       " {'generated_text': 'You are very smart. Why do you think she did this when she went there'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, num_return_sequences=2, max_length=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908be41-d4ff-4e1d-8f20-6b2bad6454ee",
   "metadata": {},
   "source": [
    "### 使用 BERT-Base-Chinese 实现中文补全\n",
    "\n",
    "![](docs/images/bert-base-chinese.png)\n",
    "\n",
    "模型主页：https://huggingface.co/bert-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8add05e-c44b-401b-8b34-0269dbf71da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94406aaf-74b4-4ef1-9f8a-5b4bd3ef767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6c28fdfe7a41ee823f92404977032c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60332\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\60332\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329af4454b174c18b1d65e8daf9a9396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26017ba2329446aa729bd15038ebc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac28a1466914062b6d7fdae007eefa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d073e741df14737952f759f70549331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill_mask = pipeline(task=\"fill-mask\", model=\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50aee00d-06f4-4e72-a79f-daa2a10ad743",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"人民[MASK]可战胜的\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423e48b8-d20b-416c-a897-c0ed773881d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5824422240257263,\n",
       "  'token': 679,\n",
       "  'token_str': '不',\n",
       "  'sequence': '人 民 不 可 战 胜 的'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493d2e4-552f-49a5-8ee3-c9b2d20d8720",
   "metadata": {},
   "source": [
    "#### 设置文本补全的条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8186a74-82f4-446a-ad27-7338613faca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"美国的首都是[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b2f2c7-baf5-44b3-8d1a-7ac9efb44fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7596936821937561,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee3557b-17ff-469f-bd23-81a54e23c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"美国的首都是[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6acb99-6192-4444-878d-04a377f3936f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7596936821937561,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'},\n",
       " {'score': 0.211267352104187,\n",
       "  'token': 511,\n",
       "  'token_str': '。',\n",
       "  'sequence': '美 国 的 首 都 是 。'},\n",
       " {'score': 0.02683423087000847,\n",
       "  'token': 8013,\n",
       "  'token_str': '！',\n",
       "  'sequence': '美 国 的 首 都 是 ！'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1d0266-a788-4196-ad4e-d30db03a94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '美国的首都是[MASK][MASK][MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67612b4-684b-43e6-9ec4-36e0b309f41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.5740302205085754,\n",
       "   'token': 5294,\n",
       "   'token_str': '纽',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 纽 [MASK] [MASK] [SEP]'}],\n",
       " [{'score': 0.49267715215682983,\n",
       "   'token': 5276,\n",
       "   'token_str': '约',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] 约 [MASK] [SEP]'}],\n",
       " [{'score': 0.9353277087211609,\n",
       "   'token': 511,\n",
       "   'token_str': '。',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] [MASK] 。 [SEP]'}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0b768-fb92-4d90-b998-43afe86fb40b",
   "metadata": {},
   "source": [
    "#### 思考：sequence 中出现的 [CLS] 和 [SEP] 是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221b545-b2be-4722-9802-134f75ec03d9",
   "metadata": {},
   "source": [
    "## 使用 AutoClass 高效管理 `Tokenizer` 和 `Model`\n",
    "\n",
    "通常，您想要使用的模型（网络架构）可以从您提供给 `from_pretrained()` 方法的预训练模型的名称或路径中推测出来。\n",
    "\n",
    "AutoClasses就是为了帮助用户完成这个工作，以便根据`预训练权重/配置文件/词汇表的名称/路径自动检索相关模型`。\n",
    "\n",
    "比如手动加载`bert-base-chinese`模型以及对应的 `tokenizer` 方法如下：\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "```\n",
    "\n",
    "以下是我们实际操作和演示：\n",
    "\n",
    "### 使用 `from_pretrained` 方法加载指定 Model 和 Tokenizer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "我们已经学会使用 Pipeline 加载支持问答任务的预训练模型，本教程代码将展示如何微调训练一个支持问答任务的模型。\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**\n",
    "\n",
    "#### 模型执行问答效果示例\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "\n",
    "# 根据自身设置下载不同的数据集\n",
    "squad_v2 = False\n",
    "# 模型名\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# 批次大小\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载数据集\n",
    "\n",
    "在本教程中，我们将使用[斯坦福问答数据集(SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)。\n",
    "\n",
    "#### SQuAD 数据集\n",
    "\n",
    "**斯坦福问答数据集(SQuAD)** 是一个阅读理解数据集，由众包工作者在一系列维基百科文章上提出问题组成。每个问题的答案都是相应阅读段落中的文本片段或范围，或者该问题可能无法回答。\n",
    "\n",
    "SQuAD2.0将SQuAD1.1中的10万个问题与由众包工作者对抗性地撰写的5万多个无法回答的问题相结合，使其看起来与可回答的问题类似。要在SQuAD2.0上表现良好，系统不仅必须在可能时回答问题，还必须确定段落中没有支持任何答案，并放弃回答。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据集下载包\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据配置判断 下载对应的数据集\n",
    "\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据集格式\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比数据集\n",
    "\n",
    "相比快速入门使用的 Yelp 评论数据集，我们可以看到 SQuAD 训练和测试集都新增了用于上下文、问题以及问题答案的列：\n",
    "\n",
    "**YelpReviewFull Dataset：**\n",
    "\n",
    "```json\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据格式：\n",
    "{'id': '5733be284776f41900661182',  \n",
    " 'title': 'University_of_Notre_Dame',  \n",
    " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',  \n",
    " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',  \n",
    " 'answers':   \n",
    "    {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "}\n",
    "- id:在数据集中的id  \n",
    "- title:文本的标题  \n",
    "- context:上下文文本  \n",
    "- question:问题  \n",
    "- ansers:标注后的结果  text：标注结果  问题结果在上下文的位置（以字符为单位）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据集具体格式\n",
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从上下文中组织回复内容\n",
    "\n",
    "答案是通过它们在文本中的起始位置（这里是515，注意：这里的515是以字符为单位），以及它们的完整文本表示的，这是上面提到的上下文的子字符串。  \n",
    "例如 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Bernadette Soubirous\n",
      "['Saint Bernadette Soubirous']\n"
     ]
    }
   ],
   "source": [
    "# 通过答案(answers)首个地址+答案长度(text)从原始文本中查找答案\n",
    "answer_start = datasets['train'][0]['answers']['answer_start'][0]\n",
    "answer_end = answer_start + len(datasets['train'][0]['answers']['text'][0])\n",
    "print(datasets['train'][0]['context'][answer_start:answer_end])\n",
    "print(datasets['train'][0]['answers']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# 导入dataset的数据类型\n",
    "from datasets import ClassLabel, Sequence, Value\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来随机抽取数据集中的数据，并且以html的方式展示出来\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= dataset.num_rows, \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = np.random.choice(dataset.num_rows, size=num_examples, replace=True).tolist()\n",
    "    # 转换成pd格式\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "\n",
    "    # for column, typ in dataset.features.items():\n",
    "    #     if isinstance(typ, ClassLabel):\n",
    "    #         df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    #     elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "    #         df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  B\n",
       "0  11  5\n",
       "1  12  6\n",
       "2  13  7\n",
       "3  14  8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform 可以将一个函数应用于 DataFrame 或 Series 的每个元素上。与 apply 方法不同，\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [5, 6, 7, 8]\n",
    "})\n",
    "df['A'] = df['A'].transform(lambda x:x+10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57277f9d5951b619008f8b69</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>The Slovenian countryside displays a variety of disguised groups and individual characters among which the most popular and characteristic is the Kurent (plural: Kurenti), a monstrous and demon-like, but fluffy figure. The most significant festival is held in Ptuj (see: Kurentovanje). Its special feature are the Kurents themselves, magical creatures from another world, who visit major events throughout the country, trying to banish the winter and announce spring's arrival, fertility, and new life with noise and dancing. The origin of the Kurent is a mystery, and not much is known of the times, beliefs, or purposes connected with its first appearance. The origin of the name itself is obscure.</td>\n",
       "      <td>Where is the most significant Slovenian festival held?</td>\n",
       "      <td>{'text': ['Ptuj'], 'answer_start': [260]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57301192a23a5019007fccef</td>\n",
       "      <td>Printed_circuit_board</td>\n",
       "      <td>At the glass transition temperature the resin in the composite softens and significantly increases thermal expansion; exceeding Tg then exerts mechanical overload on the board components - e.g. the joints and the vias. Below Tg the thermal expansion of the resin roughly matches copper and glass, above it gets significantly higher. As the reinforcement and copper confine the board along the plane, virtually all volume expansion projects to the thickness and stresses the plated-through holes. Repeated soldering or other exposition to higher temperatures can cause failure of the plating, especially with thicker boards; thick boards therefore require high Tg matrix.</td>\n",
       "      <td>What do thick boards require to resist plating failure?</td>\n",
       "      <td>{'text': ['high Tg matrix'], 'answer_start': [655]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5728c8262ca10214002da7b6</td>\n",
       "      <td>Estonia</td>\n",
       "      <td>In 2007, however, a large current account deficit and rising inflation put pressure on Estonia's currency, which was pegged to the Euro, highlighting the need for growth in export-generating industries. Estonia exports mainly machinery and equipment, wood and paper, textiles, food products, furniture, and metals and chemical products. Estonia also exports 1.562 billion kilowatt hours of electricity annually. At the same time Estonia imports machinery and equipment, chemical products, textiles, food products and transportation equipment. Estonia imports 200 million kilowatt hours of electricity annually.</td>\n",
       "      <td>When did a huge deficit and rising inflation place pressure on Estonia's currency?</td>\n",
       "      <td>{'text': ['2007'], 'answer_start': [3]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset['train'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer 是transformers库中的一个重要组件，用于加载和使用不同的预训练模型的标记化器（Tokenizer）。\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 想要使用的模型可以从 from_pretrained() 方法的预训练模型的名称或路径中推测出来。\n",
    "# 加载预训练的模型的分词器。\n",
    "# AutoTokenizer.from_pretrained 通过输入的分词器名称或路径，查找到对应的分词器。\n",
    "# 该方法根据指定的模型检查点（model_checkpoint）自动加载与之相对应的预训练分词器。这个检查点通常是一个模型的名称。\n",
    "# 如bert-base-uncased、gpt-2等。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下断言确保我们的 Tokenizers 使用的是 FastTokenizer 分词器（Rust 实现，速度和功能性上有一定优势）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用分词器进行分词处理\n",
    "\n",
    "有些模型具有可用的快速标记器，具体哪些模型有哪些模型没有可以通过大模型表查看。\n",
    "\n",
    "可以直接在两个句子上调用此分词器（一个用于答案，一个用于上下文）：\n",
    "\n",
    "双句子输入：当你传递两个句子给分词器时，它通常会将这两个句子视为一对句子。这在一些任务中很常见，比如问答任务或句子关系判断任务。\n",
    "\n",
    "分词处理：分词器会将每个句子分割成更小的单元（词、子词或符号）。对于某些模型（如BERT），它还会添加特殊的标记，如 [CLS] 和 [SEP]，以分隔句子并标记句子的开始和结束。\n",
    "\n",
    "输出：分词器的输出通常包含几个组件，最主要的是 input_ids（分词后的词汇表中的ID序列），以及可能的是 attention_mask（标识哪些ID是有意义的，哪些是填充的）和 token_type_ids（标识每个令牌属于哪个句子）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 2019, 10020, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 2019, 10020, 102]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[CLS] what is your name? [SEP] my name is anmin [SEP]\n",
      "['[CLS]', 'what', 'is', 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "# 使用默认选项时,能输入两个句子\n",
    "token = tokenizer(\"what is your name?\", \"my name is AnMin\")\n",
    "print(token)\n",
    "print(token.keys())\n",
    "print(token['input_ids'])\n",
    "# 标识哪些ID是有意义的，哪些是填充的 （1为有意义 0 为填充）\n",
    "print(token['attention_mask'])\n",
    "# decode 解码，根据tokenID映射回原始的句子\n",
    "print(tokenizer.decode(token['input_ids']))\n",
    "# convert_ids_to_tokens 根据指定得input_ids映射回原始的词\n",
    "print(tokenizer.convert_ids_to_tokens(token['input_ids'][:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2054, 2003, 2115, 2171, 1029, 102], [101, 2026, 2171, 2003, 2019, 10020, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 2054, 2003, 2115, 2171, 1029, 102]\n",
      "[101, 2026, 2171, 2003, 2019, 10020, 102]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n",
      "[CLS] what is your name? [SEP]\n",
      "[CLS] my name is anmin [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 两句话作为列表输入\n",
    "tokens = tokenizer([\"what is your name?\", \"my name is AnMin\"])\n",
    "print(tokens)\n",
    "print(tokens.keys())\n",
    "for token in tokens['input_ids']:\n",
    "    print(token)\n",
    "for token in tokens['attention_mask']:\n",
    "    print(token)\n",
    "# decode 解码，根据tokenID映射回原始的句子    \n",
    "for token in tokens['input_ids']:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 进阶操作 - 处理长文本问题\n",
    "\n",
    "**在问答预处理中的一个特定问题是如何处理非常长的文档。**\n",
    "\n",
    "在其他任务中，当文档的长度超过模型最大句子长度时，我们通常会截断它们，但在这里，如果删除上下文的一部分可能会导致我们丢失正在寻找的答案。\n",
    "\n",
    "为了解决这个问题，允许数据集中的一个（长）示例生成多个输入特征，每个特征的长度都小于模型的最大长度（或我们设置的超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "# 设置模型特征的最大输入长度(问题 加上 上下文)\n",
    "max_length = 384\n",
    "\n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "# 需要拆分上下文时，上下文的两个部分之间的授权重叠。\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超出最大长度的文本数据处理方式\n",
    "\n",
    "从训练集中找出一个超过最大长度（384）的文本 作为演示文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be95823aeaaa14008c910c',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'On April 4, 2008, Beyoncé married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan\\'s Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces Beyoncé\\'s alter ego Sasha Fierce, conceived during the making of her 2003 single \"Crazy in Love\", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving Beyoncé her third consecutive number-one album in the US. The album featured the number-one song \"Single Ladies (Put a Ring on It)\" and the top-five songs \"If I Were a Boy\" and \"Halo\". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, \"Halo\"\\'s success in the US helped Beyoncé attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful \"Sweet Dreams\", and singles \"Diva\", \"Ego\", \"Broken-Hearted Girl\" and \"Video Phone\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift\\'s \"You Belong with Me\", led to Kanye West interrupting the ceremony and Beyoncé improvising a re-presentation of Swift\\'s award during her own acceptance speech. In March 2009, Beyoncé embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.',\n",
       " 'question': 'Beyonce got married in 2008 to whom?',\n",
       " 'answers': {'text': ['Jay Z'], 'answer_start': [34]}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到一个长度大于384的文本作为例子,问题和文本加起来token小于384个\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "    # if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        # break\n",
    "    if len(tokenizer(example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# 挑选出来超过384（最大长度）的数据样例\n",
    "example = dataset[\"train\"][i]\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得问题和文本的token长度\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本预处理-截断上下文后 不保留超出部分 丢弃截断后的数据\n",
    "\n",
    "truncation：参数的选项，用于设置截取的方式\n",
    "\n",
    "- True 或 'longest_first': 这是默认选项。当输入长度超过最大长度限制时，会从最长的输入序列开始截断，直到总长度符合要求。如果有多个序列（例如，在文本对任务中），则首先截断最长的序列，如果需要，再截断第二长的序列，依此类推。\n",
    "\n",
    "- 'only_first': 当处理一对序列时（例如，在问答任务或文本对比任务中），这个选项仅截断第一个序列（通常是问题或假设），而保留第二个序列（通常是上下文或前提）的完整性。\n",
    "\n",
    "- 'only_second': 与'only_first'相反，这个选项仅截断第二个序列，保留第一个序列的完整性。在某些问答任务中，这可能有助于确保问题的完整性。\n",
    "\n",
    "- False: 不进行任何截断。如果输入序列超过了模型的最大长度限制，将会抛出错误。这个选项适用于确保输入数据完全符合模型要求的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# truncation截断 方式一 \n",
    "\n",
    "从最长的输入序列开始截断，直到总长度符合要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "384\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length = max_length, #最大的特征输入长度\n",
    "    truncation = True\n",
    "    )\n",
    "print(token.keys())\n",
    "print(len(token['input_ids']))\n",
    "print(tokenizer.decode(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "token len is 10\n",
      "data is [CLS] beyonce got married in 2008 to whom? [SEP]\n",
      "token len is 384\n",
      "data is [CLS] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    [example[\"question\"],example[\"context\"]],\n",
    "    max_length = max_length, #最大的特征输入长度\n",
    "    truncation = True\n",
    "    )\n",
    "print(tokens.keys())\n",
    "for token in tokens['input_ids']:\n",
    "    print(f\"token len is {len(token)}\")\n",
    "    print(f\"data is {tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### truncation截断 方式二 \n",
    "only_first 截断第一个序列（通常是问题或假设），保留第二个序列（通常是上下文或前提）的完整性。\n",
    "使用这种方式时候 一定要保证第二个序列的长度小于设置的max_length长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length = max_length,\n",
    "    truncation = \"only_first\"\n",
    "    )\n",
    "print(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "token len is 10\n",
      "data is [CLS] beyonce got married in 2008 to whom? [SEP]\n",
      "token len is 384\n",
      "data is [CLS] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    [example[\"question\"],example[\"context\"]],\n",
    "    max_length = max_length,\n",
    "    truncation = \"only_first\"\n",
    "    )\n",
    "print(tokens.keys())\n",
    "for token in tokens['input_ids']:\n",
    "    print(f\"token len is {len(token)}\")\n",
    "    print(f\"data is {tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### truncation截断 方式三\n",
    "'only_second': 仅截断第二个序列，保留第一个序列的完整性。在某些问答任务中，这可能有助于确保问题的完整性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "384\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length = max_length,\n",
    "    truncation = \"only_second\"\n",
    "    )\n",
    "print(token.keys())\n",
    "print(len(token['input_ids']))\n",
    "print(tokenizer.decode(token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer期望得到两个序列进行相应的处理，但如果只接收到一个序列，就会引发异常。\n",
    "tokens = tokenizer(\n",
    "    [example[\"question\"],example[\"context\"]],\n",
    "    max_length = max_length,\n",
    "    truncation = \"only_second\"\n",
    "    )\n",
    "print(tokens.keys())\n",
    "# for token in tokens['input_ids']:\n",
    "#     print(f\"token len is {len(token)}\")\n",
    "#     print(f\"data is {tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于处理截断后产生的超出部分的文本的策略\n",
    "\n",
    "参数：\n",
    "- truncation：参数的选项，设置截取的方式\n",
    "- return_overflowing_tokens： 用于设置超出设置长度后被截取后的文本，如何处理。当你设置 return_overflowing_tokens=True 时，分词器会返回一个额外的字段(overflow_to_sample_mapping)。同时input_ids和attention_mask变成列表格式，返回的包括截取前的也包括被截取后补偿的\n",
    "\n",
    "<!-- - 直接截断超出部分: 当 truncation=`only_second` 时，截断第二个序列，保留第一个序列的完整性\n",
    "- 仅截断上下文（context），保留问题（question）：设置 `return_overflowing_tokens=True` 和设置`stride`长度时 stride为截断后要补偿的 -->\n",
    "\n",
    "返回值:\n",
    "- input_ids：编码，是一个列表，包含截断前的 和 阶段后超出部分的\n",
    "- attention_mask：标识哪些ID是有意义的，哪些是填充的\n",
    "- overflow_to_sample_mapping：截断后的序列分别来自哪个原始文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n",
      "token len is 384\n",
      "data is [CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "token len is 384\n",
      "data is [CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example.keys())\n",
    "# 按照max_length截取的\n",
    "print(f\"token len is {len(tokenized_example['input_ids'][0])}\")\n",
    "print(f\"data is {tokenizer.decode(tokenized_example['input_ids'][0])}\")\n",
    "# 截取后按照doc_stride补偿的 192 = 128 + 10 + 428 - 384 + 10\n",
    "print(f\"token len is {len(tokenized_example['input_ids'][0])}\")\n",
    "print(f\"data is {tokenizer.decode(tokenized_example['input_ids'][0])}\")\n",
    "print(tokenized_example['overflow_to_sample_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此策略截断后，Tokenizer 将返回多个 `input_ids` 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到没有设置stride的时候，则默认与max_length长度相同 每次阶段出来的都是满足384个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team \n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the most by the fighting i\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example['input_ids'][0])[:100])\n",
    "print(tokenizer.decode(tokenized_example['input_ids'][1])[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 offsets_mapping 获取原始的 input_ids\n",
    "\n",
    "设置 `return_offsets_mapping=True`，将使得截断分割生成的多个 input_ids 列表中的 token，通过映射保留原始文本的 input_ids。\n",
    "\n",
    "当 return_offsets_mapping=True 时，分词器会为每个令牌返回一个元组，表示该令牌在原始未分词文本中的字符级偏移量。这个元组的形式通常是 (start, end)，\n",
    "\n",
    "其中 start 是令牌在原文中的开始位置，end 是结束位置（不包括该位置）。这里的偏移指的是 字母级别的偏移\n",
    "\n",
    "如下所示：第一个标记（[CLS]）的起始和结束字符都是（0, 0），因为它不对应问题/答案的任何部分，然后第二个标记与问题(question)的字符0到3相同.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 7),\n",
       "  (8, 11),\n",
       "  (12, 19),\n",
       "  (20, 22),\n",
       "  (23, 27),\n",
       "  (28, 30),\n",
       "  (31, 35),\n",
       "  (35, 36),\n",
       "  (0, 0),\n",
       "  (0, 2),\n",
       "  (3, 8),\n",
       "  (9, 10),\n",
       "  (10, 11),\n",
       "  (12, 16),\n",
       "  (16, 17),\n",
       "  (18, 25),\n",
       "  (26, 33),\n",
       "  (34, 37),\n",
       "  (38, 39),\n",
       "  (39, 40),\n",
       "  (41, 44),\n",
       "  (45, 53),\n",
       "  (54, 62),\n",
       "  (63, 68),\n",
       "  (69, 77),\n",
       "  (78, 80),\n",
       "  (81, 82),\n",
       "  (83, 88),\n",
       "  (89, 93),\n",
       "  (93, 96),\n",
       "  (97, 99),\n",
       "  (100, 103),\n",
       "  (104, 113),\n",
       "  (114, 119),\n",
       "  (120, 123),\n",
       "  (124, 127),\n",
       "  (128, 133),\n",
       "  (134, 140),\n",
       "  (141, 146),\n",
       "  (146, 147),\n",
       "  (148, 149),\n",
       "  (150, 152),\n",
       "  (152, 153),\n",
       "  (153, 154),\n",
       "  (154, 155),\n",
       "  (156, 161),\n",
       "  (162, 168),\n",
       "  (168, 169),\n",
       "  (170, 172),\n",
       "  (173, 182),\n",
       "  (182, 183),\n",
       "  (183, 184),\n",
       "  (185, 189),\n",
       "  (190, 194),\n",
       "  (195, 197),\n",
       "  (198, 205),\n",
       "  (206, 208),\n",
       "  (208, 209),\n",
       "  (210, 214),\n",
       "  (214, 215),\n",
       "  (216, 217),\n",
       "  (218, 220),\n",
       "  (220, 221),\n",
       "  (221, 222),\n",
       "  (222, 223),\n",
       "  (224, 229),\n",
       "  (230, 236),\n",
       "  (237, 240),\n",
       "  (241, 249),\n",
       "  (250, 252),\n",
       "  (253, 261),\n",
       "  (262, 264),\n",
       "  (264, 265),\n",
       "  (266, 270),\n",
       "  (271, 273),\n",
       "  (274, 277),\n",
       "  (278, 284),\n",
       "  (285, 291),\n",
       "  (291, 292),\n",
       "  (293, 296),\n",
       "  (297, 302),\n",
       "  (303, 311),\n",
       "  (312, 322),\n",
       "  (323, 330),\n",
       "  (330, 331),\n",
       "  (331, 332),\n",
       "  (333, 338),\n",
       "  (339, 342),\n",
       "  (343, 348),\n",
       "  (349, 355),\n",
       "  (355, 356),\n",
       "  (357, 366),\n",
       "  (367, 373),\n",
       "  (374, 377),\n",
       "  (378, 384),\n",
       "  (385, 387),\n",
       "  (388, 391),\n",
       "  (392, 396),\n",
       "  (397, 403),\n",
       "  (404, 405),\n",
       "  (405, 410),\n",
       "  (411, 413),\n",
       "  (414, 418),\n",
       "  (418, 419),\n",
       "  (419, 420),\n",
       "  (421, 428),\n",
       "  (429, 431),\n",
       "  (431, 432),\n",
       "  (432, 433),\n",
       "  (433, 436),\n",
       "  (437, 443),\n",
       "  (444, 446),\n",
       "  (447, 450),\n",
       "  (451, 456),\n",
       "  (457, 461),\n",
       "  (461, 462),\n",
       "  (463, 471),\n",
       "  (472, 476),\n",
       "  (477, 480),\n",
       "  (481, 490),\n",
       "  (491, 494),\n",
       "  (494, 495),\n",
       "  (496, 499),\n",
       "  (500, 506),\n",
       "  (507, 514),\n",
       "  (515, 518),\n",
       "  (519, 524),\n",
       "  (525, 536),\n",
       "  (537, 543),\n",
       "  (543, 544),\n",
       "  (544, 547),\n",
       "  (548, 553),\n",
       "  (554, 556),\n",
       "  (557, 560),\n",
       "  (561, 563),\n",
       "  (563, 564),\n",
       "  (565, 568),\n",
       "  (569, 574),\n",
       "  (575, 583),\n",
       "  (584, 587),\n",
       "  (588, 594),\n",
       "  (594, 595),\n",
       "  (595, 598),\n",
       "  (599, 603),\n",
       "  (604, 605),\n",
       "  (605, 611),\n",
       "  (612, 618),\n",
       "  (619, 620),\n",
       "  (620, 623),\n",
       "  (624, 625),\n",
       "  (626, 630),\n",
       "  (631, 633),\n",
       "  (634, 636),\n",
       "  (636, 637),\n",
       "  (637, 638),\n",
       "  (639, 642),\n",
       "  (643, 646),\n",
       "  (647, 650),\n",
       "  (650, 651),\n",
       "  (651, 655),\n",
       "  (656, 661),\n",
       "  (662, 663),\n",
       "  (663, 665),\n",
       "  (666, 667),\n",
       "  (668, 672),\n",
       "  (673, 674),\n",
       "  (675, 678),\n",
       "  (678, 679),\n",
       "  (680, 683),\n",
       "  (684, 685),\n",
       "  (685, 689),\n",
       "  (689, 690),\n",
       "  (690, 691),\n",
       "  (692, 701),\n",
       "  (702, 705),\n",
       "  (706, 720),\n",
       "  (721, 723),\n",
       "  (724, 732),\n",
       "  (733, 736),\n",
       "  (737, 744),\n",
       "  (744, 745),\n",
       "  (745, 752),\n",
       "  (753, 756),\n",
       "  (757, 760),\n",
       "  (761, 767),\n",
       "  (768, 770),\n",
       "  (771, 774),\n",
       "  (775, 781),\n",
       "  (781, 782),\n",
       "  (783, 784),\n",
       "  (784, 788),\n",
       "  (788, 789),\n",
       "  (789, 790),\n",
       "  (790, 791),\n",
       "  (792, 799),\n",
       "  (800, 802),\n",
       "  (803, 806),\n",
       "  (807, 809),\n",
       "  (810, 816),\n",
       "  (817, 824),\n",
       "  (825, 831),\n",
       "  (832, 836),\n",
       "  (837, 840),\n",
       "  (840, 841),\n",
       "  (841, 844),\n",
       "  (845, 852),\n",
       "  (853, 855),\n",
       "  (856, 859),\n",
       "  (860, 864),\n",
       "  (865, 869),\n",
       "  (870, 873),\n",
       "  (874, 879),\n",
       "  (880, 885),\n",
       "  (886, 892),\n",
       "  (893, 896),\n",
       "  (897, 902),\n",
       "  (902, 903),\n",
       "  (904, 906),\n",
       "  (907, 911),\n",
       "  (912, 920),\n",
       "  (921, 924),\n",
       "  (925, 935),\n",
       "  (936, 937),\n",
       "  (937, 942),\n",
       "  (943, 949),\n",
       "  (949, 950),\n",
       "  (950, 951),\n",
       "  (952, 955),\n",
       "  (956, 963),\n",
       "  (964, 965),\n",
       "  (965, 969),\n",
       "  (969, 970),\n",
       "  (970, 971),\n",
       "  (972, 973),\n",
       "  (973, 976),\n",
       "  (976, 977),\n",
       "  (977, 978),\n",
       "  (979, 980),\n",
       "  (980, 986),\n",
       "  (986, 987),\n",
       "  (987, 994),\n",
       "  (995, 999),\n",
       "  (999, 1000),\n",
       "  (1001, 1004),\n",
       "  (1005, 1006),\n",
       "  (1006, 1011),\n",
       "  (1012, 1017),\n",
       "  (1017, 1018),\n",
       "  (1018, 1019),\n",
       "  (1020, 1023),\n",
       "  (1024, 1029),\n",
       "  (1030, 1035),\n",
       "  (1036, 1039),\n",
       "  (1040, 1041),\n",
       "  (1041, 1047),\n",
       "  (1048, 1054),\n",
       "  (1054, 1055),\n",
       "  (1056, 1059),\n",
       "  (1060, 1064),\n",
       "  (1065, 1068),\n",
       "  (1068, 1070),\n",
       "  (1070, 1073),\n",
       "  (1074, 1077),\n",
       "  (1078, 1080),\n",
       "  (1080, 1086),\n",
       "  (1087, 1093),\n",
       "  (1094, 1097),\n",
       "  (1098, 1103),\n",
       "  (1103, 1104),\n",
       "  (1105, 1113),\n",
       "  (1114, 1117),\n",
       "  (1118, 1119),\n",
       "  (1119, 1124),\n",
       "  (1125, 1130),\n",
       "  (1131, 1136),\n",
       "  (1137, 1139),\n",
       "  (1139, 1141),\n",
       "  (1141, 1142),\n",
       "  (1142, 1143),\n",
       "  (1144, 1146),\n",
       "  (1147, 1150),\n",
       "  (1151, 1159),\n",
       "  (1160, 1163),\n",
       "  (1164, 1173),\n",
       "  (1174, 1176),\n",
       "  (1177, 1180),\n",
       "  (1181, 1188),\n",
       "  (1189, 1193),\n",
       "  (1193, 1194),\n",
       "  (1195, 1198),\n",
       "  (1199, 1204),\n",
       "  (1205, 1208),\n",
       "  (1209, 1212),\n",
       "  (1213, 1220),\n",
       "  (1221, 1227),\n",
       "  (1227, 1228),\n",
       "  (1229, 1238),\n",
       "  (1239, 1243),\n",
       "  (1244, 1249),\n",
       "  (1250, 1252),\n",
       "  (1253, 1256),\n",
       "  (1257, 1261),\n",
       "  (1262, 1265),\n",
       "  (1266, 1272),\n",
       "  (1273, 1278),\n",
       "  (1279, 1285),\n",
       "  (1285, 1286),\n",
       "  (1287, 1290),\n",
       "  (1291, 1295),\n",
       "  (1296, 1304),\n",
       "  (1305, 1308),\n",
       "  (1308, 1309),\n",
       "  (1310, 1316),\n",
       "  (1316, 1317),\n",
       "  (1318, 1321),\n",
       "  (1322, 1325),\n",
       "  (1326, 1330),\n",
       "  (1331, 1334),\n",
       "  (1335, 1341),\n",
       "  (1341, 1342),\n",
       "  (1343, 1345),\n",
       "  (1346, 1349),\n",
       "  (1350, 1354),\n",
       "  (1355, 1358),\n",
       "  (1359, 1364),\n",
       "  (1365, 1370),\n",
       "  (1371, 1377),\n",
       "  (1377, 1378),\n",
       "  (1379, 1382),\n",
       "  (1383, 1388),\n",
       "  (1389, 1392),\n",
       "  (1393, 1402),\n",
       "  (1403, 1406),\n",
       "  (1407, 1411),\n",
       "  (1412, 1418),\n",
       "  (1418, 1419),\n",
       "  (1420, 1430),\n",
       "  (1431, 1438),\n",
       "  (1439, 1444),\n",
       "  (1445, 1454),\n",
       "  (1455, 1460),\n",
       "  (1461, 1463),\n",
       "  (1464, 1467),\n",
       "  (1468, 1472),\n",
       "  (1472, 1473),\n",
       "  (1474, 1477),\n",
       "  (1478, 1485),\n",
       "  (1486, 1488),\n",
       "  (1489, 1492),\n",
       "  (1493, 1496),\n",
       "  (1497, 1501),\n",
       "  (1502, 1508),\n",
       "  (1509, 1514),\n",
       "  (1515, 1523),\n",
       "  (1523, 1524),\n",
       "  (1525, 1530),\n",
       "  (1531, 1535),\n",
       "  (1536, 1538),\n",
       "  (1539, 1547),\n",
       "  (1548, 1555),\n",
       "  (1556, 1559),\n",
       "  (1560, 1566),\n",
       "  (1567, 1573),\n",
       "  (1574, 1579),\n",
       "  (1579, 1580),\n",
       "  (1580, 1581),\n",
       "  (1582, 1583),\n",
       "  (1583, 1586),\n",
       "  (1587, 1593),\n",
       "  (1594, 1598),\n",
       "  (1599, 1601),\n",
       "  (1601, 1602),\n",
       "  (1602, 1603),\n",
       "  (1604, 1607),\n",
       "  (1608, 1610),\n",
       "  (1611, 1616),\n",
       "  (1617, 1621),\n",
       "  (1622, 1634),\n",
       "  (1635, 1638),\n",
       "  (1639, 1647),\n",
       "  (1648, 1651),\n",
       "  (1652, 1659),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 7),\n",
       "  (8, 11),\n",
       "  (12, 19),\n",
       "  (20, 22),\n",
       "  (23, 27),\n",
       "  (28, 30),\n",
       "  (31, 35),\n",
       "  (35, 36),\n",
       "  (0, 0),\n",
       "  (1041, 1047),\n",
       "  (1048, 1054),\n",
       "  (1054, 1055),\n",
       "  (1056, 1059),\n",
       "  (1060, 1064),\n",
       "  (1065, 1068),\n",
       "  (1068, 1070),\n",
       "  (1070, 1073),\n",
       "  (1074, 1077),\n",
       "  (1078, 1080),\n",
       "  (1080, 1086),\n",
       "  (1087, 1093),\n",
       "  (1094, 1097),\n",
       "  (1098, 1103),\n",
       "  (1103, 1104),\n",
       "  (1105, 1113),\n",
       "  (1114, 1117),\n",
       "  (1118, 1119),\n",
       "  (1119, 1124),\n",
       "  (1125, 1130),\n",
       "  (1131, 1136),\n",
       "  (1137, 1139),\n",
       "  (1139, 1141),\n",
       "  (1141, 1142),\n",
       "  (1142, 1143),\n",
       "  (1144, 1146),\n",
       "  (1147, 1150),\n",
       "  (1151, 1159),\n",
       "  (1160, 1163),\n",
       "  (1164, 1173),\n",
       "  (1174, 1176),\n",
       "  (1177, 1180),\n",
       "  (1181, 1188),\n",
       "  (1189, 1193),\n",
       "  (1193, 1194),\n",
       "  (1195, 1198),\n",
       "  (1199, 1204),\n",
       "  (1205, 1208),\n",
       "  (1209, 1212),\n",
       "  (1213, 1220),\n",
       "  (1221, 1227),\n",
       "  (1227, 1228),\n",
       "  (1229, 1238),\n",
       "  (1239, 1243),\n",
       "  (1244, 1249),\n",
       "  (1250, 1252),\n",
       "  (1253, 1256),\n",
       "  (1257, 1261),\n",
       "  (1262, 1265),\n",
       "  (1266, 1272),\n",
       "  (1273, 1278),\n",
       "  (1279, 1285),\n",
       "  (1285, 1286),\n",
       "  (1287, 1290),\n",
       "  (1291, 1295),\n",
       "  (1296, 1304),\n",
       "  (1305, 1308),\n",
       "  (1308, 1309),\n",
       "  (1310, 1316),\n",
       "  (1316, 1317),\n",
       "  (1318, 1321),\n",
       "  (1322, 1325),\n",
       "  (1326, 1330),\n",
       "  (1331, 1334),\n",
       "  (1335, 1341),\n",
       "  (1341, 1342),\n",
       "  (1343, 1345),\n",
       "  (1346, 1349),\n",
       "  (1350, 1354),\n",
       "  (1355, 1358),\n",
       "  (1359, 1364),\n",
       "  (1365, 1370),\n",
       "  (1371, 1377),\n",
       "  (1377, 1378),\n",
       "  (1379, 1382),\n",
       "  (1383, 1388),\n",
       "  (1389, 1392),\n",
       "  (1393, 1402),\n",
       "  (1403, 1406),\n",
       "  (1407, 1411),\n",
       "  (1412, 1418),\n",
       "  (1418, 1419),\n",
       "  (1420, 1430),\n",
       "  (1431, 1438),\n",
       "  (1439, 1444),\n",
       "  (1445, 1454),\n",
       "  (1455, 1460),\n",
       "  (1461, 1463),\n",
       "  (1464, 1467),\n",
       "  (1468, 1472),\n",
       "  (1472, 1473),\n",
       "  (1474, 1477),\n",
       "  (1478, 1485),\n",
       "  (1486, 1488),\n",
       "  (1489, 1492),\n",
       "  (1493, 1496),\n",
       "  (1497, 1501),\n",
       "  (1502, 1508),\n",
       "  (1509, 1514),\n",
       "  (1515, 1523),\n",
       "  (1523, 1524),\n",
       "  (1525, 1530),\n",
       "  (1531, 1535),\n",
       "  (1536, 1538),\n",
       "  (1539, 1547),\n",
       "  (1548, 1555),\n",
       "  (1556, 1559),\n",
       "  (1560, 1566),\n",
       "  (1567, 1573),\n",
       "  (1574, 1579),\n",
       "  (1579, 1580),\n",
       "  (1580, 1581),\n",
       "  (1582, 1583),\n",
       "  (1583, 1586),\n",
       "  (1587, 1593),\n",
       "  (1594, 1598),\n",
       "  (1599, 1601),\n",
       "  (1601, 1602),\n",
       "  (1602, 1603),\n",
       "  (1604, 1607),\n",
       "  (1608, 1610),\n",
       "  (1611, 1616),\n",
       "  (1617, 1621),\n",
       "  (1622, 1634),\n",
       "  (1635, 1638),\n",
       "  (1639, 1647),\n",
       "  (1648, 1651),\n",
       "  (1652, 1659),\n",
       "  (1660, 1663),\n",
       "  (1663, 1666),\n",
       "  (1666, 1671),\n",
       "  (1672, 1673),\n",
       "  (1674, 1676),\n",
       "  (1676, 1677),\n",
       "  (1677, 1689),\n",
       "  (1690, 1692),\n",
       "  (1693, 1698),\n",
       "  (1698, 1699),\n",
       "  (1699, 1700),\n",
       "  (1701, 1706),\n",
       "  (1707, 1713),\n",
       "  (1714, 1717),\n",
       "  (1718, 1721),\n",
       "  (1722, 1732),\n",
       "  (1733, 1739),\n",
       "  (1739, 1740),\n",
       "  (1741, 1743),\n",
       "  (1744, 1749),\n",
       "  (1750, 1754),\n",
       "  (1754, 1755),\n",
       "  (1756, 1763),\n",
       "  (1764, 1772),\n",
       "  (1773, 1775),\n",
       "  (1776, 1779),\n",
       "  (1780, 1781),\n",
       "  (1782, 1784),\n",
       "  (1784, 1785),\n",
       "  (1785, 1786),\n",
       "  (1786, 1787),\n",
       "  (1788, 1793),\n",
       "  (1794, 1798),\n",
       "  (1798, 1799),\n",
       "  (1800, 1803),\n",
       "  (1804, 1810),\n",
       "  (1811, 1821),\n",
       "  (1822, 1831),\n",
       "  (1832, 1839),\n",
       "  (1840, 1844),\n",
       "  (1844, 1845),\n",
       "  (1846, 1856),\n",
       "  (1857, 1859),\n",
       "  (1860, 1863),\n",
       "  (1864, 1869),\n",
       "  (1869, 1870),\n",
       "  (1871, 1879),\n",
       "  (1880, 1881),\n",
       "  (1881, 1884),\n",
       "  (1884, 1885),\n",
       "  (1885, 1886),\n",
       "  (1887, 1894),\n",
       "  (1894, 1895),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyonce'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start, end = tokenized_example['offset_mapping'][0][1]\n",
    "example[\"question\"][start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many many\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert_ids_to_tokens 和 decoder 区别：\n",
    "#### convert_ids_to_tokens：可以是token序列号\n",
    "#### decoder：是在整个字符串级别上进行的 不能多个\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 问题\n",
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助`sequence_ids`方法，我们可以方便的区分token的来源编号：\n",
    "\n",
    "- 对于特殊标记：返回None，\n",
    "- 对于正文Token：返回句子编号（从0开始编号）。\n",
    "\n",
    "综上，现在我们可以很方便的在一个输入特征中找到答案的起始和结束 Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['over 1,600'], 'answer_start': [30]}\n"
     ]
    }
   ],
   "source": [
    "# 获取回答结果\n",
    "answers = example['answers']\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取回答的结果在文本上的起始地址（以字符为单位的）\n",
    "start_char = answers['answer_start'][0]\n",
    "# 获取回答的结果在文本上的结束地址（以字符为单位的\n",
    "end_char = start_char + len(answers['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算文本内容在tokenized_example中的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 382)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当前span在文本中的起始标记索引。sequence_id为token的来源编号\n",
    "# sequence_ids是每个ID token的来源不同文本的编号：\n",
    "# 这里计算出contxt在整个文本中的位置\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "# 当前span在文本中的结束标记索引。计算是第一句的长度\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "# 找到上下文文本在整个文本中的位置，整个文本 = question + context\n",
    "token_start_index, token_end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测答案是否超出span范围（如果超出范围，该特征将以CLS标记索引标记）。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # 将token_start_index和token_end_index移动到答案的两端。\n",
    "    # 注意：如果答案是最后一个单词，我们可以移到最后一个标记之后（边界情况）。\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"答案不在此特征中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过查找 offset mapping 位置，解码 context 中的答案 \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# 直接打印 数据集中的标准答案（answer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于填充的策略\n",
    "\n",
    "- 对于没有超过最大长度的文本，填充补齐长度。\n",
    "- 对于需要左侧填充的模型，交换 question 和 context 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合以上所有预处理步骤\n",
    "\n",
    "让我们将所有内容整合到一个函数中，并将其应用到训练集。\n",
    "\n",
    "针对不可回答的情况（上下文过长，答案在另一个特征中），我们为开始和结束位置都设置了cls索引。\n",
    "\n",
    "如果allow_impossible_answers标志为False，我们还可以简单地从训练集中丢弃这些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    ''' example datasetlaoder 后的数据'''\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    # 获取token编码，这里注意需要判断是填充还是截取\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets.map 的进阶使用\n",
    "\n",
    "使用 `datasets.map` 方法将 `prepare_train_features` 应用于所有训练、验证和测试数据：\n",
    "\n",
    "- batched: 批量处理数据。\n",
    "- remove_columns: 因为预处理更改了样本的数量，所以在应用它时需要删除旧列。\n",
    "- load_from_cache_file：是否使用datasets库的自动缓存\n",
    "\n",
    "datasets 库针对大规模数据，实现了高效缓存机制，能够自动检测传递给 map 的函数是否已更改（因此需要不使用缓存数据）。如果在调用 map 时设置 `load_from_cache_file=False`，可以强制重新应用预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们的数据已经准备好用于训练，我们可以下载预训练模型并进行微调。\n",
    "\n",
    "由于我们的任务是问答，我们使用 `AutoModelForQuestionAnswering` 类。(对比 Yelp 评论打分使用的是 `AutoModelForSequenceClassification` 类）\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c982376163a4a59b8741cbc1d894a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置训练超参数（TrainingArguments）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator（数据整理器）\n",
    "\n",
    "数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 `default_data_collator`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化训练器（Trainer）\n",
    "\n",
    "为了减少训练时间（需要大量算力支持），我们不在本教程的训练模型过程中计算模型评估指标。\n",
    "\n",
    "而是训练完成后，再独立进行模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
